{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, when it failed in the middle of a batch, it might have written out the boats already (due to an accidental generator instead of list comprehension).\n",
    "This meant some duplicates. Luckily, its easy to tell, as boats are always consecutive for a day. So we just keep the last set of consecutive boats for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Results ['UDM', '20_21', '16_17_18_19_22_hpc_incomplete', '2023_07-2024_07_incomplete']\n",
      "D:\\Results\\UDM []\n",
      "D:\\Results\\20_21 []\n",
      "Processing coverage.csv\n",
      "  Saved coverage.csv\n",
      "Processing orders.csv\n",
      "  Saved orders.csv\n",
      "Processing boat_detections.csv\n",
      "  Saved boat_detections.csv\n",
      "D:\\Results\\16_17_18_19_22_hpc_incomplete []\n",
      "Processing coverage.csv\n",
      "  Saved coverage.csv\n",
      "Processing orders.csv\n",
      "  Saved orders.csv\n",
      "Processing boat_detections.csv\n",
      "  Saved boat_detections.csv\n",
      "D:\\Results\\2023_07-2024_07_incomplete []\n",
      "Processing coverage.csv\n",
      "  Saved coverage.csv\n",
      "Processing orders.csv\n",
      "  Saved orders.csv\n",
      "Processing boat_detections.csv\n",
      "  Saved boat_detections.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Results\"\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    print(root, dirs)\n",
    "    if len(dirs) > 1:\n",
    "        continue\n",
    "    for file in files:\n",
    "        if file.endswith(\".new.csv\"):\n",
    "            os.remove(os.path.join(root, file))\n",
    "        if file.endswith(\".csv\") and not file.endswith(\".new.csv\"): \n",
    "            print(f\"Processing {file}\")\n",
    "            df = pd.read_csv(os.path.join(root, file), parse_dates=['date'])\n",
    "\n",
    "            date_positions = {}\n",
    "            prev_date = None\n",
    "            start_index = 0\n",
    "\n",
    "            for i, current_date in enumerate(df['date']):\n",
    "                if current_date != prev_date:\n",
    "                    if prev_date is not None:\n",
    "                        if prev_date not in date_positions:\n",
    "                            date_positions[prev_date] = []\n",
    "                        date_positions[prev_date].append((start_index, i-1))\n",
    "                    start_index = i\n",
    "                    prev_date = current_date\n",
    "                \n",
    "            if prev_date is not None:\n",
    "                if prev_date not in date_positions:\n",
    "                    date_positions[prev_date] = []\n",
    "                date_positions[prev_date].append((start_index, len(df)-1))\n",
    "\n",
    "            non_consecutive_dates = {}\n",
    "            for date, ranges in date_positions.items():\n",
    "                if len(ranges) > 1:\n",
    "                    non_consecutive_dates[date] = ranges\n",
    "\n",
    "            for date, ranges in non_consecutive_dates.items():\n",
    "                rows = None\n",
    "                for start, end in ranges:\n",
    "                    rows = df.iloc[start:end+1] if rows is None else pd.concat([rows, df.iloc[start:end+1]])\n",
    "                    print(f\"  {start} - {end}. Length: {end-start+1}\")\n",
    "            \n",
    "            # keep only the last range\n",
    "            date_changes = df['date'] != df['date'].shift()\n",
    "            df['group_id'] = date_changes.cumsum()\n",
    "            df['max_group_id'] = df.groupby('date')['group_id'].transform('max')\n",
    "            df_last_group = df[df['group_id'] == df['max_group_id']].copy()\n",
    "            df_last_group.drop(columns=['group_id', 'max_group_id'], inplace=True)\n",
    "            # save the last range\n",
    "            df_last_group.to_csv(os.path.join(root, file), index=False)\n",
    "            print(f\"  Saved {file}\")\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D:\\Results ['UDM', '20_21', '16_17_18_19_22_hpc_incomplete', '2023_07-2024_07_incomplete']\n",
    "D:\\Results\\UDM []\n",
    "D:\\Results\\20_21 []\n",
    "Processing coverage.csv\n",
    "  Saved coverage.csv\n",
    "Processing orders.csv\n",
    "  Saved orders.csv\n",
    "Processing boat_detections.csv\n",
    "  71212 - 71401. Length: 190\n",
    "  71505 - 71694. Length: 190\n",
    "  71402 - 71429. Length: 28\n",
    "  71695 - 71722. Length: 28\n",
    "  71430 - 71504. Length: 75\n",
    "  71723 - 71797. Length: 75\n",
    "  Saved boat_detections.csv\n",
    "D:\\Results\\16_17_18_19_22_hpc_incomplete []\n",
    "Processing coverage.csv\n",
    "  Saved coverage.csv\n",
    "Processing orders.csv\n",
    "  Saved orders.csv\n",
    "Processing boat_detections.csv\n",
    "C:\\Users\\turner30\\AppData\\Local\\Temp\\ipykernel_28600\\1675975194.py:13: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
    "  df = pd.read_csv(os.path.join(root, file), parse_dates=['date'])\n",
    "  23350 - 24250. Length: 901\n",
    "  26109 - 27009. Length: 901\n",
    "  24251 - 24841. Length: 591\n",
    "  27010 - 27600. Length: 591\n",
    "  24842 - 24846. Length: 5\n",
    "  27601 - 27605. Length: 5\n",
    "  24847 - 25202. Length: 356\n",
    "  27606 - 27984. Length: 379\n",
    "  25203 - 25916. Length: 714\n",
    "  27985 - 28698. Length: 714\n",
    "  25917 - 26108. Length: 192\n",
    "  28699 - 28890. Length: 192\n",
    "  50271 - 51096. Length: 826\n",
    "  56888 - 57713. Length: 826\n",
    "  51097 - 52326. Length: 1230\n",
    "  57714 - 58943. Length: 1230\n",
    "  52327 - 52538. Length: 212\n",
    "  58944 - 59155. Length: 212\n",
    "  52539 - 53510. Length: 972\n",
    "  59156 - 60127. Length: 972\n",
    "  53511 - 53529. Length: 19\n",
    "  60128 - 60146. Length: 19\n",
    "  53530 - 54496. Length: 967\n",
    "  60147 - 61113. Length: 967\n",
    "  54497 - 54897. Length: 401\n",
    "  61114 - 61514. Length: 401\n",
    "  54898 - 55787. Length: 890\n",
    "  61515 - 62404. Length: 890\n",
    "  55788 - 56887. Length: 1100\n",
    "  62405 - 63504. Length: 1100\n",
    "  74200 - 74780. Length: 581\n",
    "  79004 - 79584. Length: 581\n",
    "  74781 - 75089. Length: 309\n",
    "  79585 - 79893. Length: 309\n",
    "  75090 - 75371. Length: 282\n",
    "  79894 - 80175. Length: 282\n",
    "  75372 - 75547. Length: 176\n",
    "  80176 - 80351. Length: 176\n",
    "  75548 - 75787. Length: 240\n",
    "  80352 - 80591. Length: 240\n",
    "  75788 - 76598. Length: 811\n",
    "  80592 - 81402. Length: 811\n",
    "  76599 - 77528. Length: 930\n",
    "  81403 - 82332. Length: 930\n",
    "  77529 - 78287. Length: 759\n",
    "  82333 - 83091. Length: 759\n",
    "  78288 - 78334. Length: 47\n",
    "  83092 - 83138. Length: 47\n",
    "  78335 - 79003. Length: 669\n",
    "  83139 - 84157. Length: 1019\n",
    "  121621 - 121870. Length: 250\n",
    "  122510 - 122759. Length: 250\n",
    "  121871 - 121992. Length: 122\n",
    "  122760 - 122881. Length: 122\n",
    "  121993 - 122257. Length: 265\n",
    "  122882 - 123153. Length: 272\n",
    "  122258 - 122496. Length: 239\n",
    "  123154 - 123804. Length: 651\n",
    "  122497 - 122509. Length: 13\n",
    "  123805 - 125221. Length: 1417\n",
    "  135370 - 136010. Length: 641\n",
    "  141108 - 141748. Length: 641\n",
    "  136011 - 136376. Length: 366\n",
    "  141749 - 142114. Length: 366\n",
    "  136377 - 137332. Length: 956\n",
    "  142115 - 143070. Length: 956\n",
    "  137333 - 137765. Length: 433\n",
    "  143071 - 143503. Length: 433\n",
    "  137766 - 138727. Length: 962\n",
    "  143504 - 144465. Length: 962\n",
    "  138728 - 140405. Length: 1678\n",
    "  144466 - 146143. Length: 1678\n",
    "  140406 - 141107. Length: 702\n",
    "  146144 - 146845. Length: 702\n",
    "  189595 - 189971. Length: 377\n",
    "  200711 - 201087. Length: 377\n",
    "  189972 - 190218. Length: 247\n",
    "  201088 - 201334. Length: 247\n",
    "  190219 - 190812. Length: 594\n",
    "  201335 - 201928. Length: 594\n",
    "  190813 - 191016. Length: 204\n",
    "  201929 - 202132. Length: 204\n",
    "  191017 - 192402. Length: 1386\n",
    "  202133 - 203518. Length: 1386\n",
    "  192403 - 193473. Length: 1071\n",
    "  203519 - 204589. Length: 1071\n",
    "  193474 - 194696. Length: 1223\n",
    "  204590 - 205812. Length: 1223\n",
    "  194697 - 194740. Length: 44\n",
    "  205813 - 205856. Length: 44\n",
    "  194741 - 195132. Length: 392\n",
    "  205857 - 206248. Length: 392\n",
    "  315616 - 316323. Length: 708\n",
    "  316683 - 317390. Length: 708\n",
    "  316324 - 316682. Length: 359\n",
    "  317391 - 317749. Length: 359\n",
    "  Saved boat_detections.csv\n",
    "D:\\Results\\2023_07-2024_07_incomplete []\n",
    "Processing coverage.csv\n",
    "  Saved coverage.csv\n",
    "Processing orders.csv\n",
    "  Saved orders.csv\n",
    "Processing boat_detections.csv\n",
    "  Saved boat_detections.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Boats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
